# üìä Implementaci√≥n y An√°lisis de Linear Discriminant Analysis (LDA)

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.3+-orange.svg)](https://scikit-learn.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

## üìù Descripci√≥n

Implementaci√≥n completa y documentada de **Linear Discriminant Analysis (LDA)** con comparaci√≥n exhaustiva contra **Principal Component Analysis (PCA)**. Este proyecto incluye validaci√≥n estad√≠stica de supuestos, evaluaci√≥n con clasificadores, y an√°lisis de eigenvectores discriminantes.

### üéØ Objetivos del Proyecto

- Implementar LDA desde cero usando scikit-learn
- Comparar t√©cnicas supervisadas (LDA) vs no supervisadas (PCA)
- Validar supuestos estad√≠sticos (normalidad, homocedasticidad)
- Evaluar rendimiento con m√∫ltiples clasificadores
- Interpretar eigenvectores discriminantes

---

## üìÇ Estructura del Proyecto

```
.
‚îú‚îÄ‚îÄ Implementacion_LDA_Sustentacion_2_50_IC.ipynb  # Notebook principal
‚îî‚îÄ‚îÄ README.md                                       # Este archivo
```

---

## üîß Instalaci√≥n y Configuraci√≥n

### Requisitos Previos

- Python 3.8 o superior
- pip (gestor de paquetes)
- Jupyter Notebook o VS Code con extensi√≥n de Python

### Instalaci√≥n de Dependencias

```bash
pip install scikit-learn scipy pandas numpy matplotlib seaborn pingouin -q
```

O usando requirements.txt:

```bash
pip install -r requirements.txt
```

### Ejecuci√≥n del Notebook

```bash
jupyter notebook Implementacion_LDA_Sustentacion_2_50_IC.ipynb
```

O abrir directamente en VS Code.

---

## üìä Datasets Utilizados

### 1. Wine Dataset üç∑
- **Muestras**: 178
- **Caracter√≠sticas**: 13 (caracter√≠sticas qu√≠micas)
- **Clases**: 3 (tipos de vino italiano)
- **Prop√≥sito**: Demostrar LDA con m√∫ltiples clases (visualizaci√≥n 2D)

### 2. Breast Cancer Dataset üè•
- **Muestras**: 569
- **Caracter√≠sticas**: 30 (caracter√≠sticas de tumores)
- **Clases**: 2 (benigno/maligno)
- **Prop√≥sito**: Demostrar LDA binario (visualizaci√≥n 1D)

---

## üßÆ Metodolog√≠a

### 1. Preprocesamiento
- Divisi√≥n train/test (70/30) con estratificaci√≥n
- Estandarizaci√≥n (Œº=0, œÉ=1) usando `StandardScaler`

### 2. Reducci√≥n de Dimensionalidad
- **LDA**: Maximiza separaci√≥n entre clases
  - Wine: 2 componentes (k-1, donde k=3 clases)
  - Breast Cancer: 1 componente (k-1, donde k=2 clases)
- **PCA**: Maximiza varianza total
  - Ambos datasets: 2 componentes para comparaci√≥n

### 3. Validaci√≥n Estad√≠stica

#### Test de Mardia (Normalidad Multivariada)
Eval√∫a si los datos siguen distribuci√≥n normal multivariada.

**Hip√≥tesis:**
- H‚ÇÄ: Los datos siguen una distribuci√≥n normal multivariada
- H‚ÇÅ: Los datos no siguen una distribuci√≥n normal multivariada

**Criterio:** p-value > 0.05 ‚Üí Aceptar normalidad

#### Test de Box's M (Homocedasticidad)
Verifica igualdad de matrices de covarianza entre clases.

**Hip√≥tesis:**
- H‚ÇÄ: Las matrices de covarianza son iguales
- H‚ÇÅ: Las matrices de covarianza son diferentes

**Criterio:** p-value > 0.05 ‚Üí Aceptar homocedasticidad

### 4. Evaluaci√≥n con Clasificadores

Se eval√∫an las proyecciones LDA y PCA usando:

- **SVM (Support Vector Machine)** con kernel RBF
- **Regresi√≥n Log√≠stica** (baseline lineal)

**M√©tricas:**
- Accuracy (exactitud)
- Matriz de confusi√≥n
- Classification report (precision, recall, f1-score)

---

## üìà Resultados Esperados

### Wine Dataset (3 clases)

| M√©todo | SVM Accuracy | LR Accuracy | Dimensiones |
|--------|-------------|-------------|-------------|
| **LDA** | ~98-100% | ~97-99% | 2D |
| **PCA** | ~95-97% | ~94-96% | 2D |

**Conclusi√≥n:** LDA supera a PCA en tareas de clasificaci√≥n.

### Breast Cancer Dataset (2 clases)

| M√©todo | SVM Accuracy | LR Accuracy | Dimensiones |
|--------|-------------|-------------|-------------|
| **LDA** | ~96-98% | ~95-97% | 1D |
| **PCA** | ~93-95% | ~92-94% | 2D |

**Conclusi√≥n:** LDA con 1 componente puede superar a PCA con 2 componentes.

---

## üîç An√°lisis de Eigenvectores

Los eigenvectores discriminantes revelan las caracter√≠sticas m√°s importantes:

### Wine Dataset - Top Caracter√≠sticas (LD1)

1. **Flavonoids** (~0.85) - Mayor peso discriminante
2. **Proline** (~0.24)
3. **Color intensity** (~-0.56) - Direcci√≥n opuesta

**Interpretaci√≥n:** Los flavonoides son el factor qu√≠mico principal que diferencia entre tipos de vino.

### Breast Cancer Dataset - Top Caracter√≠sticas (LD1)

Las caracter√≠sticas de textura y √°rea de los tumores suelen tener los pesos m√°s altos.

---

## üß™ Validaci√≥n de Supuestos

### Resultados T√≠picos

#### Normalidad (Test de Mardia)
- **Wine**: Generalmente cumple normalidad multivariada
- **Breast Cancer**: Puede mostrar desviaciones leves

#### Homocedasticidad (Box's M)
- **Wine**: Matrices de covarianza similares
- **Breast Cancer**: Puede mostrar heterogeneidad

> **Nota:** LDA es robusto a violaciones moderadas de estos supuestos, especialmente con muestras grandes.

---

## üìö Fundamentos Te√≥ricos

### Linear Discriminant Analysis (LDA)

LDA busca la proyecci√≥n que maximiza:

$$J(w) = \frac{w^T S_B w}{w^T S_W w}$$

Donde:
- $S_B$ = Matriz de dispersi√≥n **between-class** (entre clases)
- $S_W$ = Matriz de dispersi√≥n **within-class** (dentro de clases)
- $w$ = Vector de proyecci√≥n √≥ptimo

### Componentes Discriminantes

Cada componente discriminante es una combinaci√≥n lineal:

$$LD_i = w_{i1} \cdot x_1 + w_{i2} \cdot x_2 + \ldots + w_{ip} \cdot x_p$$

### Limitaciones de LDA

1. **M√°ximo k-1 componentes** (k = n√∫mero de clases)
2. Asume **normalidad multivariada**
3. Asume **homocedasticidad** (covarianzas iguales)
4. **Lineal**: No captura relaciones no lineales

---

## üöÄ Extensiones Posibles

### 1. Quadratic Discriminant Analysis (QDA)
Para datos con matrices de covarianza diferentes por clase.

```python
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
qda = QuadraticDiscriminantAnalysis()
```

### 2. Regularized LDA
Para datos de alta dimensi√≥n (p >> n).

```python
lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')
```

### 3. Kernel LDA
Para fronteras de decisi√≥n no lineales.

### 4. Cross-Validation
Validaci√≥n cruzada k-fold para mayor robustez.

```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(lda, X, y, cv=5)
```

---

## üî¨ Tecnolog√≠as Utilizadas

| Librer√≠a | Versi√≥n | Prop√≥sito |
|----------|---------|-----------|
| **scikit-learn** | ‚â•1.3 | LDA, PCA, clasificadores |
| **numpy** | ‚â•1.24 | Operaciones num√©ricas |
| **pandas** | ‚â•2.0 | Manipulaci√≥n de datos |
| **matplotlib** | ‚â•3.7 | Visualizaci√≥n |
| **seaborn** | ‚â•0.12 | Gr√°ficos estad√≠sticos |
| **scipy** | ‚â•1.10 | Tests estad√≠sticos |

---

## üìñ Referencias

### Art√≠culos Fundamentales
1. **Fisher, R.A. (1936)**. "The use of multiple measurements in taxonomic problems". *Annals of Eugenics*, 7(2), 179-188.
   - Art√≠culo original que introduce LDA

2. **Duda, R.O., Hart, P.E., & Stork, D.G. (2001)**. *Pattern Classification* (2nd ed.). Wiley.
   - Cap√≠tulo 3: Linear Discriminant Functions

### Libros Recomendados
3. **Hastie, T., Tibshirani, R., & Friedman, J. (2009)**. *The Elements of Statistical Learning*. Springer.
   - Secci√≥n 4.3: Linear Discriminant Analysis

4. **James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013)**. *An Introduction to Statistical Learning*. Springer.
   - Cap√≠tulo 4: Classification

### Documentaci√≥n
5. [Scikit-learn LDA Documentation](https://scikit-learn.org/stable/modules/lda_qda.html)
6. [Scipy Statistical Tests](https://docs.scipy.org/doc/scipy/reference/stats.html)

---

## üë• Autores

- **Milton Nicolas Pirazan Forero** - *Implementaci√≥n y documentaci√≥n*

---

## üîÑ Historial de Versiones

- **v1.0.0** (Nov 2025) - Implementaci√≥n completa con documentaci√≥n
  - ‚úÖ LDA y PCA implementados
  - ‚úÖ Tests estad√≠sticos (Mardia, Box's M)
  - ‚úÖ Evaluaci√≥n con SVM y Regresi√≥n Log√≠stica
  - ‚úÖ An√°lisis de eigenvectores
  - ‚úÖ Documentaci√≥n completa en notebook